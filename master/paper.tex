\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Path2Vec: An NLP Based Mechanism for File Access Pattern Detection\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
    Many scientific applications spend a large proportion of the execution time to access files. 
    To narrow the increasing gap between computing and I/O performance, several optimization techniques were adopted, 
    such as data prefetching and data layout optimization. However, the effectiveness of these optimization processes heavily 
    depends on the understanding of the I/O behavior. Traditionally, spatial locality and temporal locality
    are mainly considered for data prefetching and scheduling policy. Whereas for some complex workloads, the file access pattern 
    is hard to reveal due to ... 
    We propose Path2Vec, an embedding approach to transform filenames to vectors, thus allowing further analysis of access patterns. 
    To evaluate the effectiveness of this mechanism, we implemented a 

\end{abstract}

\begin{IEEEkeywords}

\end{IEEEkeywords}

\section{Introduction}
Distributed file systems such as HDFS\cite{HDFS}, GlusterFS[], Ceph\cite{Ceph}, Lustre[] \textcolor{red}{etc}. have been widely deployed as back-end storage systems for parallel/distributed applications.
with the data scale continuously increased, i/o access latency between computing nodes and storage servers has become one of the severest bottlenecks in this big data era.
In order to narrow this performance gap, data caching/prefetching in file systems is one of the most well-known optimization techniques.
Caches(burst buffers?) with non-volatile memories(NVMs) and solid state drives(SSDs) are placed at different levels of the I/O stack to hide network and disk latency.

\textcolor{red}{Traditional methodoligies: mainly apllying spatial and temporal locality}
For purpose of caching data from lower storage in predictable manner, access pattern detection is essential.
Traditionally, spatial locality and temporal locality are most generally considered facts. 
\textit{Readahead} is a basic mechanism implemented by most file systems\cite{WhyDoes} including linux kernel, which prefeches constant size of data from a opened file to the cache. 
For workloads excuting sequential read, this approach benefits a lot due to the simple and definite pattern.
Unfortunately, many applications especially sientific ones usually do not follow sequential behavior(\textcolor{yellow}{IO acceleration with patter detection}). 
\textcolor{red}{How about small files workloads?}


\textcolor{red}{Curently works}




\textcolor{red}{Briefly introducing your work to address the drawbacks mentioned above.}

The main contributions of this paper can be summerized as follow.
\begin{itemize}
    \item We design and implement a filename embedding model named path2vec, 
    which traverses the user's whole directory and captures naming conventions and semantic relations among directories and files.
    \item A tracing translator is inserted into GlusterFS's storage stack, which is able to collect file access histories and represent these histories as vectors.

\end{itemize}

\section{Related Work}
N-gram staging: high level works on data libraries
C-miner: low level on block-rate optimization.
meta data access optimizations.

\section{Background and Motivation}
\subsection{Introduction to GlusterFS}
GlusterFS is a scalable distributed filesystem which is suitable for data-intensive workloads such as cloud storage and high performance computing.
It aggregates various storage servers over Ethernet or Infiniband RDMA interconnect into one large parallel network file system.
Servers are typically deployed as storage bricks.


Since version 3.7, the newly implemented feature \textit{tiring} enables different sotrage types to be used by the same logical volume.
As fig shows, the two types of bricks are classified as \textit{cold} and \textit{hot}. The hot group acts as a cache for the cold group. 
With fine-tuned prefeching and cache replacing policy, large propotion of I/O requests are forwarded to \textit{hot} bricks, thus significantly decreasing access latency.
In this case, we can reframe(\textcolor{red}{EDIT}) caching/prefeching optimization tasks to binary classification problems,
where the objective is to dynamically decide whether a file is \textit{hot} or \textit{cold}.

\subsection{Word Embedding}
The concept word embedding is a set of language models for distributed representations of words in \textit{Natural Language Processing}(NLP), 
aiming at mapping natural words to real number vectors based on context provided by specific corpora.
These mapping mechanisms include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic methods etc. \textcolor{blue}{refs}


The most widely utilized model is \textit{skip-gram algorithm}\cite{SkipGram}, 
which trains a neural network to predict whether a word is likely to show up near another.
Given a word vocabulary of size $W$, where a word is identified by $w \in {1,\dots,W}$, 
and a corpus consists of a sequence of words $w_1, \dots, w_T$, the objective of skip-gram algorithm is to maximize the following log probability:
\begin{equation}
\frac{1}{T}\sum_{t=1}^{T}\sum_{-c \leq j \leq c,j \neq 0} \log p(w_{t+j} | w_t),
\end{equation}
Where $w_t$ denotes the target word at position $t$ in the training corpus and $c$ is the size of context window.
A basic definition of $p(w_t+j | w_t)$ is the softmax function:
\begin{equation}
%    p(w_O | w_I) = \frac{\exp{v'_{w_O}^{\top}}{b}
\end{equation}
However, it's generally impractical to calculate softmax function due to the tremendous \textbf{(EDIT)}computational complexity,
as the size of vocabulary may be too large.
To estimate ...

\subsection{Gated Recurrent Neural Networks}
RNNs(\textit{Recurrent Neural Networks}) have been widely utilized and shown exellent performance in multiple NLP tasks,
such as speech recognition, machine translation and text emotion classification etc.
The RNN is an extension of a conventional feedforward neural network, which is able to process a variable-length sequence input.
Given a sequence input $x = (x_1, x_2, \dots, x_T)$, the RNN updates its recurrent hidden state $h_t$ by
\begin{equation}
    \mathbf{h_t}=
        \begin{cases}
            0, &t=0 \\
            \phi(\mathbf{h_{t-1}, \mathbf{x_t}}), &\text{otherwise}
        \end{cases}
\end{equation}
Where $\phi$ is a nonlinear function calculating current hidden state $h_t$ based on last state $h_{t-1}$ and current input $x_t$.

Gated recurrent unit(GRU)\textcolor{blue}{ref} is a gating mechanism in RNNs, 
which is like a long short-term memory(LSTM)\textcolor{blue}{ref} with forget gate but has fewer parameters.
See \textcolor{blue}{fig} 



\section{Design}
\subsection{Path Embedding Model}
Since words embedding mechanisms have significantly promoted the performance of plenty of NLP tasks, 
we assume that these methodologies are suitable in analytics and optimizations for posix file systems as well.
\textit{e.g.}, given paths like \textit{/a/b/c/}, \textit{/a/b/d}, we can claim(\textcolor{red}{EDIT}) that \textit{a} and \textit{b},
\textit{b} and \textit{c} are parent-child pairs, while \textit{c} and \textit{d} are a pair of brothers with the same parent \textit{b}.
Or, intuitively, they are logically close to each other in this directory tree. 
This basic relation is analogous to 2-Grams (\textcolor{red}{EDIT}) in natural language models.

Formally, given a directory tree accessed by a specific workload such as compiling, 
it is assumed that the naming conventions and directory structure are regularly formed by producers or users.
Thus it is possible to form a vocabulary and train an embedding model to represent each directory/file name as a vector.

To generate corpora for training purpose,
we recursively traverse the target directory from root entry and print the paths into a txt file line by line.
After that, the Skip-gram algorithm is invoked to train a embedding model, 
which finally provides a vocabulary for all the occured directory/file names and a table consists of vectors for these names.


\subsection{Access Pattern Analytics with Gated Recurrent Neural Network}
\subsubsection{Data pre-processing}
To collect file access histories, we insert a trace translator into GlusterFS.
So far, we mainly focus on file \textit{open} operation in order to detect when, how and what files does the workload start to access during its lifetime,
attemtpting to reveal its underlying access patterns beyond basic locality. 
As \textbf{fig} illustrated, the components of parameter \textit{path} in each \textit{open} request are transformed to vectors and summed up.\textcolor{red}{edit} 

\textcolor{blue}{A figure here}

Finally, a sequence of vectors $\mathbf{x}=(\mathbf{x_1}, \dots, \mathbf{x_T})$ representing access history of size $T$ have been generated.

\subsubsection{GRU-based model building\textcolor{red}{edit}}
In our design, we utilize a single-layer GRU to calculate and update the hidden state, which implicitly represents the access pattern of the target workloads. 
Given dataset $\mathbf{x} = (\mathbf{x}_1,\dots,\mathbf{x}_T)$ representing a sequence of file access history excuted by the target workload, 
we take these vectors as input sequence for the GRU and train the model. 

At time $t$, the activation $h_t^{(j)}$, which denotes the $j$th element of the hidden state $\mathbf{h}_t$, is a linear interpolation between the previous activation $h_{t-1}^{(j)}$ and the candidate activation $\tilde{h}_t^{(j)}$:
\begin{equation}
    h_t^{(j)} = (1-z_t^{(j)}) h_{t-1}^{(j)} + z_t^{(j)}\tilde{h}_t^{(j)},
\end{equation}
Where $z_t$ is denoted as an \textit{update gate}, calculated by a sigmoid function over input $\mathbf{x_t}$ and previous state $\mathbf{h_{t-1}}$:
\begin{equation}
z_t^{(j)} = \sigma(W_z \mathbf{x}_t + U_z \mathbf{h}_{t-1})^{(j)}
\end{equation}
The candidate acitivation $\tilde{h}_t^{(j)}$ is computed by:
\begin{equation}
    \tilde{h}_t^{(j)} = \tanh(W \mathbf{x}_t + U (\mathbf{r}_t \odot \mathbf{h}_{t-1}))^{(j)}
\end{equation}
Where $\mathbf{r}_t$ are \textit{reset gates} and $\odot$ is a pointwise multiplication(Hadarmard product).
The reset gate is calculated by
\begin{equation}
    r_t^{(j)} = \sigma(W_r \mathbf{x}_t + \mathbf{U}_r \mathbf{h}_{t-1})^{(j)}
\end{equation}
For purpose of discriminating files between \textit{hot} and \textit{cold}, 
our goal is to train the GRU to provide appropriate hidden states for classification. 
We denote $N$ as the approximation of the number of files fit in the cache,
thus at time $t$, the following $N$ files(vectors) are labeled as \textit{hot}, i.e., positive samples, 
while the others are labeled as \textit{cold}(negative).
As long as $\mathbf{h}_t$ is calculated, the objective is to maximize the log probability
\begin{equation}
    \label{eq.obj}
    \sum_{i=1}^N \log P(\mathbf{x}_{t+j}|\mathbf{h}_t)
\end{equation}
Where we calculate the probability $P(\mathbf{x}_{t+j}|\mathbf{h}_t)$ by
\begin{equation}
    P(\mathbf{x}_{t+j}|\mathbf{h}_t) = \sigma(\mathbf{x}^\top_t \mathbf{h}_t)
\end{equation}
Thus we obtain the loss function for training
\begin{equation}
    arg\,max L(\theta) = \sum_{i=1}^N \log \sigma(\mathbf{x}^\top_t \mathbf{h}_t)
\end{equation}
Where the parameter $\theta = \{W_z, U_z, W, U, W_r, U_r\}$.
We train this GRU with BPTT(backpropagation through time)\textcolor{red}{ref}.



\section{Evaluation}
\subsection{Experiment and Parameter Settings}
We choose compiling workloads for experiments and collect a few different repositories from Github,
including \textcolor{red}{edit} files and occupying volume of \textcolor{red}{edit} gigabytes in total.
We split these repositories into two groups, 10 arbitrary repositories for training and the others for testing. 

To evaluate the performance of our model, 


\subsection{Results}





%\subsection{\textcolor{red}{EDIT}Similarity Experiment for Directory/File Names}
%In this part, we picked up \textcolor{red}{edit} most frequently occured names from vocabulary, which is generated from the training group of repositories with Skip-gram algorithm.
%We utilize PCA(\textit{Principal Component Analysis}) to project these \textcolor{red}{edit} vectors(\textcolor{red}{EDIT}) into a 2-D graph.
%As fig(\textbf{refhere}) illustrates, 

%\subsection{\textcolor{red}{edit}{Evaluation for dynamically file classification}}
\section{Conclusion}
In this paper, we systematically discuss the similarity between natural language processing and posix file systems,
and propose a new approch to represent posix file/direcory names with Skip-gram algorithm. 
With this embedding mechanism, we map the file/directory names into a high dimension vector space, 
where the relation between files can be dipicted by dot production of these two vectors.  

Based on this embedding mechanism, we exploit a single-layer GRNN(Gated Recurrent Neural Network) taking file access history as sequence input.
With well trained GRU, we are able to encode the access pattern to the hidden state and dynamically decide whether a file is \textit{hot} or \textit{cold},
thus providing hints for caching/prefeching and cache replacing policies.

There are several works to accomplish in the future. First, the model should be deployed in tiering translator in GlusterFS,
thus making it possible to exploit our model in real fils system for optimization purpose. 
Also, our work should be scaled to more real world workloads rather than compiling.
\bibliographystyle{plain}
\bibliography{paper}

\end{document}
